# Diagnosis-of-Wilson-s-Disease-Using-Deep-Learning-and-Comparing-Neural-Architecture-Performance


Abstract— Wilson's disease is a rare genetic disorder characterized by the accumulation of copper in the liver and brain, leading to various neurological and hepatic symptoms. Diagnosing Wilson's disease typically relies on medical imaging techniques, such as magnetic resonance imaging (MRI). In this groundbreaking research paper, we present the first-ever application of deep learning techniques to Wilson's disease MRI for the purpose of identifying a specific neurological manifestation known as "panda face." The panda face phenomenon in Wilson's disease MRI refers to a distinctive pattern of hyperintensity in the midbrain on T2-weighted images, resembling the facial features of a panda. To address this unique diagnostic challenge, we conducted a comparative study involving various deep neural networks. Our study involved the collection and preprocessing of a substantial dataset of Wilson's disease MRI scans, followed by an extensive evaluation of different deep learning architectures. We compared the performance of these models, including DETR FPN (our model) in terms of accuracy loss and IoU in detecting the panda face pattern. The results of our research provide valuable insights into the potential of deep learning for improving the accuracy and efficiency of Wilson's disease diagnosis. This innovative approach has the potential to revolutionize the field of medical imaging and enhance the early detection of Wilson's disease, ultimately leading to more effective treatments and improved patient outcomes. 

![DL+WD1](https://github.com/user-attachments/assets/527da79b-ab17-4d79-88fc-91fb1a2bc5be)


Figure 1: Image A (on the left) appears to have a higher contrast between the grey and white matter of the brain. It is likely a T1-weighted image, which typically shows grey matter as lighter and white matter as darker. The cerebrospinal fluid (CSF) appears dark.
Image B (on the right) seems to be a T2-weighted image, where the CSF and other fluids appear bright, and the grey matter is darker compared to the white matter. This type of imaging is sensitive to fluid, which makes it appear bright.

![DL+WD2](https://github.com/user-attachments/assets/a89b5ae9-bfd1-4b3d-ab82-5cf603875e8f)


Figure 2: The output of from DETR with FPN detecting the panda face

III. DATASET
The data is meticulously categorized into two distinct groups. The first category, Wilson Positive - Panda Face, is a compilation of brain MRI scans showcasing the impact of Wilson's disease, each meticulously annotated. This category boasts a comprehensive collection of 101 samples. These images serve as a crucial resource for the development and refinement of computer vision algorithms tailored for the identification of Wilson's disease in brain scans.
On the contrasting end, the second category, Wilson Negative - No Panda Face, is dedicated to healthy brain T2 and T1 MRI scans, similarly enriched with annotations. This category, intended to serve as a benchmark for contrast, comprises a set of 55 samples. However, it's noteworthy that 101 sample category exhibits a relative scarcity of images, emphasizing the unique challenges associated with procuring a substantial dataset of Wilson disease affected brain scans with detailed annotations. Despite this scarcity, these images are indispensable for creating a robust and differentiating model that can accurately discern between healthy and Wilson-affected brain scans and avoid false positive. For pretraining we have used T2 and T1 MRI data[14]
A. Deep Learning and Wilson’s Disease
Wilson's disease (WD) typically relies on medical imaging techniques, such as magnetic resonance imaging (MRI). Furthermore, there is a lack of comparative studies on this topic. [2] In this study, the authors introduce and validate an effective approach for automatic WD classification, utilizing data derived from whole brain segmentation volumes and cortical thickness measurements gathered from T1-weighted magnetic resonance images (MRIs).
The study compares three established supervised machine learning algorithms: support vector machine (SVM), linear discriminant analysis (LDA), and logistic regression (LR), in the context of WD classification. The validation analysis made use of 51 images, with 27 originating from WD patients and 24 from healthy controls matched for age. The authors implemented univariate feature selection to discard features not relevant to the classification and to maintain those that enhance classification performance. They adopted a dual-layer leave-one-out cross-validation approach, utilizing the inner layer for the optimal parameter estimation and the outer layer for evaluating the performance of the classification.
The experimental results revealed that SVM, when using volume features, significantly surpassed both LDA and LR in performance. It achieved an impressive overall accuracy of 96.1%, along with a sensitivity of 92.6% and a specificity of 100%. Interestingly, LR also reached this level of optimum classification performance when utilizing a combination of volume and thickness features as input. This study introduces a novel, non-invasive MRI-based tool for automated WD detection, presenting a significant advancement in the field. In contrast our studies reply on deep learning specifically object detection [3].
There were two significant study based on functional magnetic resonance imaging (rs-fMRI) [4, 9] In this [4] research, a cutting-edge classification technique is introduced, aiming to accurately distinguish WD subjects from their healthy counterparts. This method encompasses the integration of two disparate features, extracted from resting-state functional magnetic resonance imaging (rs-fMRI) through the application of Regional Homogeneity (ReHo) and Amplitude of Low-Frequency Fluctuations (ALFF). A total of 32 patients diagnosed with WD, alongside 26 healthy controls (HC) matched for both age and gender, participated in this study. All participants underwent both three-dimensional T1 and rs-fMRI scans, from which data was procured. Following this, two-sample t-tests were utilized to analyze the differences in ReHo and ALFF values between the two groups. Subsequently, a support vector machine (SVM) classifier, equipped with an RBF kernel, was implemented to assess the influence of abnormal rs-fMRI parameters in differentiating WD patients from HC.
The findings from the experiments indicate an elevation of ALFF in the pallidum and thalamus, alongside a decrease in the orbital part of the inferior frontal gyrus, cingulum, and medial frontal gyrus in individuals with WD. Concurrently, an increase in ReHo was observed in the inferior frontal gyrus, postcentral gyrus, and insula of WD patients, with a decrease noted in the superior frontal gyrus. The application of leave-one-out cross-validation demonstrated that the integration of features (ReHo + ALFF) rendered an impressive classification outcome, boasting a 91.6% accuracy, 91.07% sensitivity, and 86.93% specificity. This paper's results highlight the presence of ReHo and ALFF abnormalities in WD patients, affirming the robustness of combining ReHo and ALFF features for precise identification of individuals with Wilson's disease. This study [9] aimed to explore the potential of amplitude of low-frequency fluctuations (ALFF) as a biomarker for WD diagnosis. The study encompassed 30 healthy controls (HCs) and 37 WD patients (WDs), with resting-state functional magnetic resonance imaging (rs-fMRI) data collected for analysis.
The ALFF values were derived from preprocessing of the rs-fMRI data, and the study sought to differentiate WD patients from HCs based on these values. Four clusters with abnormal ALFF-z values were identified through between-group comparisons, which served as the basis for developing three machine learning models: Random Forest (RF), Support Vector Machine (SVM), and Logistic Regression (LR). The abnormal ALFF z-values were also combined with additional data including volume information, clinical variables, and imaging features for further model development.
Significant differences were observed in the ALFF z-values between WDs and HCs across four clusters, including areas in the cerebellar region, left caudate nucleus, bilateral thalamus, and right caudate nucleus. Models trained with data from clusters 2, 3, and 4 achieved area under curve (AUC) values greater than 0.80 in both training and test sets. However, only the AUC values for models trained with Cluster 4 data exhibited statistical significance in the Delong test. The LR and RF models, in particular, showed superior performance compared to the SVM model in terms of AUC values.
In the test set, LR and RF models trained with Cluster 3 data demonstrated high specificity, sensitivity, and accuracy. However, the DeLong test revealed no statistically significant difference in AUC values between models that integrated multi-modal information and those that did not. Despite this, LR models trained with multimodal information and Cluster 4 data, as well as LR and RF models trained with multimodal information and Cluster 3 data, exhibited high accuracy, specificity, and sensitivity. The study highlights the effectiveness of utilizing ALFF values based on the thalamus or caudate nucleus as markers for distinguishing WD patients from HCs. The integration of multimodal information, though, did not substantially enhance the classification performance of the models compared to those without such integration.
Unlike our study which is based on MRI in the paper [5], the authors introduce an image processing algorithm designed to detect the Kayser-Fleischer ring in the eye's cornea, a prevalent symptom of the rare genetic disorder Wilson Disease, caused by impaired copper excretion and subsequent accumulation in tissues. This copper buildup leads to oxidative processes in affected organs, visibly manifesting as a golden-brown, orange, or greyish pigmentation in the cornea, known as the Kayser-Fleischer ring. This ring is a crucial diagnostic indicator of Wilson Disease, especially in individuals presenting neurological disorders. The authors' algorithm, rooted in image processing, analyzes eye images through a segmentation algorithm to detect the Kayser-Fleischer ring, offering a non-invasive and automated screening method. This novel diagnostic tool seeks to enhance the accuracy of current practices, minimizing potential interpretation errors and aiding doctors in diagnosing the pathology. Unlike our research which relies on State of the art transformers model [8,10] This paper is closed to our study [6] where the authors introduce a computer-aided design-based automated classification strategy that leverages optimized transfer learning (TL), employing two innovative paradigms: MobileNet and Visual Geometric Group-19 (VGG-19)[12] Additionally, the authors set these TL systems in comparison against a machine learning (ML) paradigm.
Through the application of four-fold augmentation, the study demonstrates that VGG-19 outperforms MobileNet [ 11]achieving accuracy and area under the curve (AUC) pairs of 95.46 ± 7.70 % and 0.932 (p < 0.0001), in comparison to MobileNet's 86.87 ± 2.23 % and 0.871 (p < 0.0001). Moreover, when compared against the ML-based soft classifier – Random Forest, MobileNet and VGG-19 exhibited improvements of 3.4% and 13.5%, respectively. This showcases the potential of these advanced TL paradigms in enhancing the accuracy and efficiency of diagnosing Wilson's disease through MRI analysis.
We've carried out comprehensive research and discovered that there haven't been many any studies utilizing computer vision for Wilson's disease detection in brain MRI. Therefore, our research is truly unique in its kind.


VII.	REFERENCES

[1]	https://doi.org/10.1097/00005792-199205000-00004 
[2]	L.  Zou, Y. Song, J. Chu and X. Tang, “Whole brain volume and cortical thickness based automatic classification of Wilson’s disease.” in 2019 IEEE International Conference on Systems, Man and Cybernetics, SMC, pp. 819-824 , Oct. 2019.
[3]	L. Jiao, F. Zhang, F. Liu, S. Yang, L. Li, Z. Feng and R. Qu, “A survey of deep learning-based object detection.” IEEE access, 7, pp.128837-128868, 2019. 
[4]	Y. Wu, H. Kan, S. Hu, H. Lu and L. Jin, “Predictive Value of Different rs-fMRI Parameters in Wilson Disease based on SVM.” in 2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference, pp. 1534-1538, Oct. 2022.
[5]	R. Morello, C. De Capua, L. Fabbiano and G. Vacca, “Image-based detection of Kayser-Fleischer ring in patient with Wilson disease.” in 2013 IEEE International Symposium on Medical Measurements and Applications, pp. 101-106, 2013.
[6]	L. Saba,  M. Agarwal, S.S. Sanagala, S. K. Gupta, G. R. Sinha, A. M. Johri, N.N. Khanna, S. Mavrogeni, J. R. Laird, G.J.E.L Pareek and M. Miner, “Brain MRI‐based Wilson disease tissue classification: An optimised deep transfer learning approach.” Electronics Letters, 56(25), pp.1395-1398, 2020.
[7]	D.C. Preston, “Magnetic resonance imaging (mri) of the brain and spine: Basics,” MRI Basics, Case Med, 30, pp.1-6, 2006.
[8]	S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F.S. Khan and M. Shah, “Transformers in vision: A survey.” ACM computing surveys (CSUR), 54(10s), pp.1-41, 2022.
[9]	B. Zhang, J. Peng, H. Chen and W. Hu, “Machine learning for detecting Wilson's disease by amplitude of low-frequency fluctuation.” Heliyon, 9(7), 2023.
[10]	F. Shamshad, S. Khan, S.W. Zamir, M.H. Khan, M. Hayat, F.S.Khan and H. Fu, “Transformers in medical imaging: A survey.” Medical Image Analysis, p.102802, 2023.
[11]	A.G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision applications.” arXiv preprint arXiv:1704.04861, 2017.
[12]	K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv:1409.1556, 2014.
[13]	C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y.  Zhou, W. Li and P.J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer.” The Journal of Machine Learning Research, 21(1), pp.5485-5551, 2020.
[14]	X. Chen, L. Qu, Y. Xie, S. Ahmad and P.T. Yap, “A paired dataset of T1-and T2-weighted MRI at 3 Tesla and 7 Tesla.” Scientific Data, 10(1), p.489, 2023.
[15]	S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection with region proposal networks.” Advances in neural information processing systems, 28, 2015.
[16]	T.Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C.L. Zitnick , “Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014.” in 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 (pp. 740-755). Springer International Publishing.
[17]	T.Y. Lin,  P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection.” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2117-2125, 2017.
[18]	S. Ren, K. He, R.B. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection with region proposal networks.” Advances in neural information processing systems, 28, 2015.
[19]	Z. Cai and N. Vasconcelo, “Cascade R-CNN: High quality object detection and instance segmentation.” in IEEE transactions on pattern analysis and machine intelligence, 43(5), pp.1483-1498, 2019.
[20]	T.Y. Lin, P. Goyal, R.B. Girshick, K. He, and P. Doll´ar , “Focal loss for dense object detection.” in Proceedings of the IEEE international conference on computer vision, pp. 2980-2988, 2017.
[21]	X. Zhou, D. Wang, and P. Kr¨ahenb¨uhl, “Objects as points.” arXiv:1904.07850, 2019.
[22]	Z. Tian, C. Shen, H. Chen, and T. He, “FCOS: Fully convolutional one-stage object detection.” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9627-9636, 2019. 
[23]	S. Zhang, C. Chi, Y. Yao, Z. Lei, and S.Z. Li, “Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection.” arXiv:1912.02424, 2019.
[24]	N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, “End-to-end object detection with transformers,” In European conference on computer vision, pp. 213-229, Cham: Springer International Publishing, Aug. 2020.
[25]	J. Terven and D. Cordova-Esparza, “A comprehensive review of YOLO: From YOLOv1 to YOLOv8 and beyond,” arXiv preprint arXiv:2304.00501, 2023.
[26]	Z. Zou, K. Chen, Z. Shi, Y. Guo, and J. Ye, “Object detection in 20 years: A survey,” Proceedings of the IEEE, 2023.
[27]	E. Arkin, N. Yadikar, X. Xu, A. Aysa, and K. Ubul, “A survey: Object detection methods from CNN to transformer,” Multimedia Tools and Applications, 82(14), pp.21353-21383, 2023.
[28]	Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage object detection,” in Proceedings of the IEEE/CVF international conference on computer vision, pp. 9627-9636, 2019.
[29]	Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer, and C.J. Hsieh, “Large batch optimization for deep learning: Training bert in 76 minutes,” arXiv preprint arXiv:1904.00962, 2019. 

